{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNaw9NKykXoe6OWbqQBrQDT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1_--ApdwrEzr","executionInfo":{"status":"ok","timestamp":1737479362602,"user_tz":-60,"elapsed":22018,"user":{"displayName":"Lucia Galiero","userId":"07205723943900468988"}},"outputId":"3b702aee-3b97-4a43-a5d7-b35d64e23726"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import csv\n","import os\n","import pandas as pd"],"metadata":{"id":"bDmE17fjrbjV","executionInfo":{"status":"ok","timestamp":1737479382555,"user_tz":-60,"elapsed":643,"user":{"displayName":"Lucia Galiero","userId":"07205723943900468988"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Paths to folders containing English and Italian text files\n","tsvfile = '/content/drive/MyDrive/Tico19_NMT_LLM/TICO19_En-It.tsv'\n","test_set_en = '/content/drive/MyDrive/Tico19_NMT_LLM/Sets/test_en.txt'\n","test_set_it= '/content/drive/MyDrive/Tico19_NMT_LLM/Sets/test_it.txt'\n","tuning = '/content/drive/MyDrive/Tico19_NMT_LLM/Sets/tuning.txt'\n","training_en = '/content/drive/MyDrive/Tico19_NMT_LLM/Sets/training_en.txt'\n","training_it ='/content/drive/MyDrive/Tico19_NMT_LLM/Sets/training_it.txt'\n","sets_loc = '/content/drive/MyDrive/Tico19_NMT_LLM/Sets'"],"metadata":{"id":"RoJE665ht5X0","executionInfo":{"status":"ok","timestamp":1737479384374,"user_tz":-60,"elapsed":495,"user":{"displayName":"Lucia Galiero","userId":"07205723943900468988"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["import csv\n","\n","def process_tsv(input_tsv, output_dir):\n","    # Initialize lists to store rows\n","    every_third = []\n","    every_fourth = []\n","    leftovers = []\n","\n","    # Read the input TSV file\n","    with open(input_tsv, 'r', encoding='utf-8') as infile:\n","        reader = csv.DictReader(infile, delimiter='\\t')\n","        rows = list(reader)\n","\n","    # Extract every third sentence pair (up to 100)\n","    for i in range(0, len(rows), 3):\n","        if len(every_third) < 100:\n","            every_third.append(rows[i])\n","        else:\n","            leftovers.append(rows[i])\n","\n","    # Extract every fourth sentence from the remaining rows (up to 50)\n","    remaining_rows = [row for row in rows if row not in every_third]\n","    for i in range(0, len(remaining_rows), 4):\n","        if len(every_fourth) < 50:\n","            every_fourth.append(remaining_rows[i])\n","        else:\n","            leftovers.append(remaining_rows[i])\n","\n","    # Extract all leftovers that were not touched\n","    untouched_rows = [row for row in remaining_rows if row not in every_fourth]\n","\n","    # Write the first 100 sentence pairs to text files\n","    with open(test_set_en, 'w', encoding='utf-8') as src_100, open(test_set_it, 'w', encoding='utf-8') as tgt_100:\n","        for row in every_third:\n","            src_100.write(row['sourceString'] + '\\n')\n","            tgt_100.write(row['targetString'] + '\\n')\n","\n","    # Write the 50 additional sentences to a text file\n","    with open(tuning, 'w', encoding='utf-8') as src_50:\n","        for row in every_fourth:\n","            src_50.write(row['sourceString'] + '\\n')\n","\n","    # Write leftover sentences to text files\n","    with open(training_en, 'w', encoding='utf-8') as src_leftover, open(training_it, 'w', encoding='utf-8') as tgt_leftover:\n","        for row in untouched_rows:\n","            src_leftover.write(row['sourceString'] + '\\n')\n","            tgt_leftover.write(row['targetString'] + '\\n')\n","\n","# Example usage:\n","process_tsv(tsvfile, sets_loc)"],"metadata":{"id":"jtZmCVVT7wwA","executionInfo":{"status":"ok","timestamp":1737479398562,"user_tz":-60,"elapsed":3602,"user":{"displayName":"Lucia Galiero","userId":"07205723943900468988"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["def check_shared_sentences(file1, file2, file3=None):\n","    # Read sentences from the first file\n","    with open(file1, 'r', encoding='utf-8') as f1:\n","        sentences_file1 = set(f1.readlines())\n","\n","    # Read sentences from the second file\n","    with open(file2, 'r', encoding='utf-8') as f2:\n","        sentences_file2 = set(f2.readlines())\n","\n","    # Read sentences from the third file, if provided\n","    if file3:\n","        with open(file3, 'r', encoding='utf-8') as f3:\n","            sentences_file3 = set(f3.readlines())\n","    else:\n","        sentences_file3 = set()\n","\n","    # Find shared sentences between the files\n","    shared_file1_file2 = sentences_file1.intersection(sentences_file2)\n","    shared_file1_file3 = sentences_file1.intersection(sentences_file3)\n","    shared_file2_file3 = sentences_file2.intersection(sentences_file3)\n","\n","    # Display shared items, if any\n","    if shared_file1_file2:\n","        print(\"Shared sentences between file 1 and file 2:\")\n","        for sentence in sorted(shared_file1_file2):\n","            print(sentence.strip())\n","\n","    if shared_file1_file3:\n","        print(\"\\nShared sentences between file 1 and file 3:\")\n","        for sentence in sorted(shared_file1_file3):\n","            print(sentence.strip())\n","\n","    if shared_file2_file3:\n","        print(\"\\nShared sentences between file 2 and file 3:\")\n","        for sentence in sorted(shared_file2_file3):\n","            print(sentence.strip())\n","\n","    if not (shared_file1_file2 or shared_file1_file3 or shared_file2_file3):\n","        print(\"No shared sentences found.\")\n","\n","# Example usage:\n","# Files in English\n","file1 = test_set_en\n","file2 = tuning\n","file3 = training_en\n","\n","\n","check_shared_sentences(file1, file2, file3)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ztBw6-6sFu6g","executionInfo":{"status":"ok","timestamp":1737479408588,"user_tz":-60,"elapsed":260,"user":{"displayName":"Lucia Galiero","userId":"07205723943900468988"}},"outputId":"7e730287-4358-40ac-9669-b79b9dcdd990"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["No shared sentences found.\n"]}]},{"cell_type":"code","source":["def verify_sentence_coverage(tsvfile, test_set_en, tuning, training_en):\n","    # Read all sentences from the CSV's sourceString column\n","    with open(tsvfile, 'r', encoding='utf-8') as infile:\n","        reader = csv.DictReader(infile, delimiter=',')\n","        source_sentences = set(row['sourceString'] for row in reader)\n","\n","    # Read all sentences from the three txt files\n","    def read_sentences(file_path):\n","        with open(file_path, 'r', encoding='utf-8') as f:\n","            return set(f.readlines())\n","\n","    top_100_sentences = read_sentences(test_set_en)\n","    one_in_four_sentences = read_sentences(tuning)\n","    leftover_sentences = read_sentences(training_en)\n","\n","    # Combine sentences from all three files\n","    combined_sentences = top_100_sentences.union(one_in_four_sentences).union(leftover_sentences)\n","\n","    # Strip whitespace for consistent matching\n","    combined_sentences = {sentence.strip() for sentence in combined_sentences}\n","    source_sentences = {sentence.strip() for sentence in source_sentences}\n","\n","    # Find missing sentences\n","    missing_sentences = source_sentences - combined_sentences\n","\n","    # Output results\n","    if missing_sentences:\n","        print(\"The following sentences from sourceString are missing:\")\n","        for sentence in sorted(missing_sentences):\n","            print(sentence)\n","    else:\n","        print(\"All sentences from sourceString are covered in the text files.\")\n","\n","verify_sentence_coverage(tsvfile, test_set_en, tuning, training_en)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":326},"id":"RAoHAvBiNYRc","executionInfo":{"status":"error","timestamp":1737479439121,"user_tz":-60,"elapsed":300,"user":{"displayName":"Lucia Galiero","userId":"07205723943900468988"}},"outputId":"46bd81bc-cdd5-4576-f169-b01f54b05579"},"execution_count":7,"outputs":[{"output_type":"error","ename":"KeyError","evalue":"'sourceString'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-e6a52fa8bd8a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All sentences from sourceString are covered in the text files.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mverify_sentence_coverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtsvfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set_en\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_en\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-7-e6a52fa8bd8a>\u001b[0m in \u001b[0;36mverify_sentence_coverage\u001b[0;34m(tsvfile, test_set_en, tuning, training_en)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtsvfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0msource_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sourceString'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Read all sentences from the three txt files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-e6a52fa8bd8a>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtsvfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0msource_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sourceString'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Read all sentences from the three txt files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'sourceString'"]}]},{"cell_type":"code","source":["def check_sentence_length(file_paths, word_limit=80):\n","    \"\"\"\n","    Checks if there are sentences longer than a specified word limit in the given files.\n","\n","    Args:\n","        file_paths (list): List of file paths to check.\n","        word_limit (int): Maximum allowed word count for a sentence. Default is 80.\n","\n","    Returns:\n","        dict: A dictionary with file paths as keys and lists of long sentences as values.\n","    \"\"\"\n","    long_sentences = {}\n","\n","    for file_path in file_paths:\n","        with open(file_path, 'r', encoding='utf-8') as file:\n","            sentences = file.readlines()\n","            # Find sentences exceeding the word limit\n","            long_sentences[file_path] = [sentence.strip() for sentence in sentences if len(sentence.split()) > word_limit]\n","\n","    return long_sentences\n","\n","\n","# Example usage:\n","file_paths = [test_set_en, test_set_it, tuning, training_en, training_it]\n","\n","long_sentences = check_sentence_length(file_paths)\n","\n","# Print results\n","for file, sentences in long_sentences.items():\n","    if sentences:\n","        print(f\"Sentences longer than {60} words found in {file}:\")\n","        for sentence in sentences:\n","            print(f\"- {sentence}\")\n","    else:\n","        print(f\"No sentences longer than {60} words found in {file}.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bR7zBg1C2znU","executionInfo":{"status":"ok","timestamp":1737479851490,"user_tz":-60,"elapsed":398,"user":{"displayName":"Lucia Galiero","userId":"07205723943900468988"}},"outputId":"24b89f0a-aea4-4591-def1-21ab9ea45c86"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["No sentences longer than 60 words found in /content/drive/MyDrive/Tico19_NMT_LLM/Sets/test_en.txt.\n","No sentences longer than 60 words found in /content/drive/MyDrive/Tico19_NMT_LLM/Sets/test_it.txt.\n","No sentences longer than 60 words found in /content/drive/MyDrive/Tico19_NMT_LLM/Sets/tuning.txt.\n","No sentences longer than 60 words found in /content/drive/MyDrive/Tico19_NMT_LLM/Sets/training_en.txt.\n","No sentences longer than 60 words found in /content/drive/MyDrive/Tico19_NMT_LLM/Sets/training_it.txt.\n"]}]}]}